<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Why Replicating Movement Is Harder Than Generating Text</title>
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" href="/images/favicon.ico">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link rel="stylesheet" href="/styles/katex.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Roboto+Condensed:ital,wght@0,100..900;1,100..900&family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined">
    <link rel="stylesheet" href="/styles/kCLLP63cz-.css">
    <link rel="me" href="mailto:asanka@gmail.com">
    <link rel="me" href="https://github.com/asankah">
    <meta name="generator" content="Pandoc + Eleventy">
  </head>
  <body>
    <header class="container">
      <nav class="navlike">
        <h1><a href="/">Strong Opinions Weekly Held</a></h1>
        <span class="fill"></span>
        <h2><a href="/about/me/">About</a></h2>
        <h2><a href="/tags/">Tags</a></h2>
        <h2><a href="/feed.xml"><img src="/images/rss.svg"></a></h2>
      </nav>
    </header>
    <main>
      <article class="container">
        <header>
          <h1>Why Replicating Movement Is Harder Than Generating Text</h1>
          <div class="navlike">
            <span>
          <span class="date">Posted on January 1, 2025</span><span class="author">&nbsp;by Asanka Herath</span></span>
            <span class="fill"></span>
          </div>
          <div class="navlike">
            <span class="readingtime">Estimated reading time is 33 minutes.</span>
            <span class="fill"></span>
          </div>
        </header>
        <p>Artificial intelligence (AI) has made incredible strides in recent
          years, achieving remarkable feats in areas like text generation and
          information processing. AI can now write stories, translate languages,
          and even compose music with impressive proficiency. However, when it
          comes to replicating mechanical motion, particularly the fluid and
          adaptable movements of humans and animals, AI still has a long way to
          go. This article delves into the reasons behind this disparity,
          exploring the unique challenges that make replicating motion a more
          complex endeavor than generating text. To gather the information for
          this article, research was conducted in five key steps:</p>
        <ol type="1">
          <li>Finding articles and research papers discussing the challenges of
            replicating mechanical motion artificially.<br>
          </li>
          <li>Finding articles and research papers discussing the advancements in
            AI for text generation and information processing tasks.<br>
          </li>
          <li>Finding articles and research papers comparing the complexities of
            mechanical motion and information processing tasks from a computational
            perspective.<br>
          </li>
          <li>Finding articles and research papers discussing the limitations of
            current robotics technology in replicating complex mechanical
            motions.<br>
          </li>
          <li>Finding articles and research papers discussing the role of embodied
            cognition in mechanical motion and how it differs from information
            processing tasks.</li>
        </ol>
        <h2 id="the-complexity-of-mechanical-motion"><strong>The Complexity of
Mechanical Motion</strong></h2>
        <p>While generating text involves manipulating symbols and patterns
          within a defined set of rules, replicating mechanical motion requires
          navigating the complexities of the physical world. Here’s why this is so
          challenging:</p>
        <ul>
          <li><strong>The Physical World Is Unpredictable:</strong> Unlike the
            digital realm, where data is structured and predictable, the physical
            world is full of unexpected variables. A robot navigating a room must
            contend with uneven surfaces, obstacles, and even sudden changes in
            lighting or the movement of objects <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.<br>
          </li>
          <li><strong>Infinite Degrees of Freedom:</strong> Human movement is
            incredibly versatile. Our bodies have numerous joints and muscles that
            allow for a vast range of motion. Replicating this flexibility in a
            robot requires complex mechanics and sophisticated control systems <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.<br>
          </li>
          <li><strong>Real-Time Adaptation:</strong> Humans and animals seamlessly
            adapt their movements to changing circumstances. A robot trying to mimic
            this must be able to perceive its environment, process information, and
            adjust its actions in real-time, a task that demands significant
            computational power and advanced algorithms <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.<br>
          </li>
          <li><strong>Embodied Cognition:</strong> Our understanding of the world
            is deeply intertwined with our physical experiences. We learn to move
            and interact with our surroundings through a process of embodied
            cognition, where our bodies and senses play a crucial role. Replicating
            this in a robot requires more than just programming; it necessitates a
            deeper understanding of how the brain and body work together to generate
            and control movement <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
        </ul>
        <h2 id="advancements-in-ai-for-text-and-information-processing"><strong>Advancements
in AI for Text and Information Processing</strong></h2>
        <p>In contrast to the challenges of replicating motion, AI has excelled
          in text generation and information processing tasks. This success can be
          attributed to several factors:</p>
        <ul>
          <li><strong>Structured Data:</strong> Text and information are typically
            represented in a structured format, making it easier for AI systems to
            analyze and manipulate <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.<br>
          </li>
          <li><strong>Clear Rules and Patterns:</strong> Languages and information
            systems follow specific rules and patterns, which AI models can learn
            and apply with increasing accuracy <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.<br>
          </li>
          <li><strong>Abundant Training Data:</strong> The availability of vast
            amounts of text and data has enabled AI models to learn and improve
            their performance significantly <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.<br>
          </li>
          <li><strong>Advancements in Algorithms:</strong> Deep learning and other
            AI techniques have proven highly effective in processing and generating
            text, leading to breakthroughs in natural language processing and
            machine translation <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</li>
        </ul>
        <h2 id="comparing-complexities-motion-vs.-information"><strong>Comparing
Complexities: Motion vs. Information</strong></h2>
        <p>From a computational perspective, replicating mechanical motion
          presents a different set of challenges compared to information
          processing.</p>
        <ul>
          <li><strong>Continuous vs. Discrete:</strong> Motion is continuous,
            involving a smooth flow of movements and adjustments. Information
            processing, on the other hand, often deals with discrete units of data
            <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.<br>
          </li>
          <li><strong>Physical Constraints:</strong> Replicating motion requires
            dealing with the physical constraints of the real world, such as
            gravity, friction, and the limitations of robotic materials. Information
            processing operates within the more forgiving confines of the digital
            world <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.<br>
          </li>
          <li><strong>Sensorimotor Integration:</strong> Motion involves the
            intricate coordination of sensory input and motor output. AI systems for
            motion must be able to process sensory data, make decisions, and send
            commands to actuators in a seamless and efficient manner <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</li>
        </ul>
        <h2 id="limitations-of-current-robotics-technology"><strong>Limitations
of Current Robotics Technology</strong></h2>
        <p>Despite advancements in robotics, current technology still faces
          limitations in replicating complex motions. These limitations can be
          grouped into several key areas:</p>
        <h3 id="actuator-and-material-limitations"><strong>Actuator and Material
Limitations</strong></h3>
        <ul>
          <li>Artificial muscles and actuators often lack the power, efficiency,
            and flexibility of their biological counterparts. Researchers are
            exploring various electrically powered artificial muscle fiber actuation
            mechanisms, such as electrothermal, electrochemical, and dielectric
            actuation, to achieve the desired characteristics of high efficiency,
            precise control, miniaturization, and seamless integration with
            electronic components <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</li>
        </ul>
        <h3 id="sensor-integration-and-perception"><strong>Sensor Integration
and Perception</strong></h3>
        <ul>
          <li>Integrating sensors into robots to provide real-time feedback and
            environmental awareness remains a challenge. To accurately replicate
            human motion, robots need to perceive and react to their surroundings
            effectively, especially in dynamic environments with limited fields of
            view <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>.</li>
        </ul>
        <h3 id="motion-capture-technology"><strong>Motion Capture
Technology</strong></h3>
        <p>Accurately capturing and analyzing human motion data is crucial for
          replicating it in robots. Various motion capture technologies are used,
          each with its own advantages and disadvantages:</p>
        <ul>
          <li><strong>Optical Tracking Systems:</strong> These systems use markers
            to track movement. Passive marker systems are susceptible to occlusion
            and require post-processing, while active marker systems, though more
            precise, can be cumbersome due to wires and battery packs. Markerless
            vision-based systems offer a wireless solution but are sensitive to
            lighting and noise <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.<br>
          </li>
          <li><strong>Inertial Sensor Tracking Systems:</strong> These systems are
            wireless and offer fast calibration but are sensitive to magnetic fields
            and can produce noisy data <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.<br>
          </li>
          <li><strong>Magnetic Marker Tracking Systems:</strong> These systems are
            also wireless and can be used indoors or outdoors but have a limited
            range and are sensitive to magnetic fields <a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>.</li>
        </ul>
        <h3 id="motion-planning-and-control"><strong>Motion Planning and
Control</strong></h3>
        <p>Replicating human-like motion requires sophisticated motion planning
          and control algorithms. Some of the key approaches and challenges
          include:</p>
        <ul>
          <li><strong>Dynamic Movement Primitives (DMPs):</strong> DMPs are used
            to generate and control complex movements by encoding trajectories and
            adapting them to new situations. However, challenges remain in
            replicating the planarity characteristic of human hand motion,
            especially in cluttered environments <a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>.<br>
          </li>
          <li><strong>Attractive and Repulsive Vector Fields:</strong> These
            fields are used in robot motion planning to guide robots towards goals
            and away from obstacles. However, replicating the smooth and efficient
            obstacle avoidance strategies of humans remains a challenge <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.<br>
          </li>
          <li><strong>Hybrid Joints/Operational-Space Approach:</strong> This
            approach combines joint-level and operational-space control to enhance
            the human-likeness of robot motion. It aims to achieve both accuracy in
            task execution and natural-looking movements <a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>.<br>
          </li>
          <li><strong>Dynamic Models of Human Arm:</strong> Researchers are using
            dynamic models that simulate the capabilities of the human arm to
            generate biologically-inspired arm movements <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>.</li>
        </ul>
        <h3 id="anatomical-and-kinematic-differences"><strong>Anatomical and
Kinematic Differences</strong></h3>
        <ul>
          <li>Retargeting human motions to robots is complex due to fundamental
            differences in their anatomies, kinematics, and motion dynamics. Robots
            have rigid bodies, different form factors, and distinct physical
            limitations compared to humans. Directly mapping human motion to robot
            actuators often leads to unnatural and suboptimal robot behavior. For
            example, replicating the motion of a human touching their head requires
            careful consideration of the robot’s kinematics to ensure the robot hand
            reaches the intended target <a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>.</li>
        </ul>
        <h3 id="robotics-in-co-packing-operations"><strong>Robotics in
Co-packing Operations</strong></h3>
        <p>The use of robots in co-packing operations provides a practical
          example of the challenges and advancements in robotics technology.</p>
        <ul>
          <li><strong>Types of Robots:</strong> Various types of robots are used
            in co-packing, including collaborative robots (cobots), articulated
            robots, SCARA robots, and Delta robots. Each type has its own strengths
            and limitations in terms of flexibility, speed, precision, and reach <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.<br>
          </li>
          <li><strong>Role of AI:</strong> AI is playing a crucial role in
            improving the efficiency and intelligence of robotic systems in
            co-packing. AI enables robots to learn from their environment and make
            autonomous decisions, further streamlining the process <a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>.</li>
        </ul>
        <h3 id="challenges-in-unstructured-environments"><strong>Challenges in
Unstructured Environments</strong></h3>
        <p>Developing robots that can operate effectively in unstructured
          environments, such as homes, presents significant challenges.</p>
        <ul>
          <li><strong>Dexterity and Accuracy:</strong> Robots need to possess high
            levels of dexterity and accuracy to perform complex tasks in these
            environments <a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>.<br>
          </li>
          <li><strong>Physical Dimensions and Mobility:</strong> The physical
            dimensions and mobility of robots can be obstructive in a home setting
            <a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>.<br>
          </li>
          <li><strong>Personality and Aesthetics:</strong> The personality and
            aesthetics of robots are important for human interaction and acceptance
            in a home environment <a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>.<br>
          </li>
          <li><strong>Communication and Functionality:</strong> Robots need to be
            able to communicate effectively and perform a wide range of functions to
            be useful in a home setting <a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>.</li>
        </ul>
        <h3 id="energy-and-resource-management"><strong>Energy and Resource
Management</strong></h3>
        <p>Robots require significant power to operate, and current battery
          technology often limits their endurance and range of motion. For robots
          to be truly autonomous, they need to be self-sufficient in terms of
          energy and resource management. This may involve developing robots that
          can operate in different gravitational environments, adapt to different
          terrains, and even generate their own energy from the environment <a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>.</p>
        <h3 id="focus-of-current-research"><strong>Focus of Current
Research</strong></h3>
        <p>Current research on human-like arm motion generation has primarily
          focused on single-arm reaching movements and biomimetic-based methods,
          with less attention given to manipulation, obstacle avoidance
          mechanisms, and dual-arm motion generation. This limited focus restricts
          the ability of current approaches to fully respect human behavioral and
          neurological key features and limits their generalizability to a wider
          range of tasks <a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>.</p>
        <h2 id="embodied-cognition-and-the-difference-it-makes"><strong>Embodied
Cognition and the Difference It Makes</strong></h2>
        <p>Embodied cognition plays a crucial role in how humans learn and
          perform movements. This concept suggests that our understanding of the
          world is shaped by our physical experiences and interactions with our
          environment <a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>.</p>
        <ul>
          <li><strong>Learning Through Interaction:</strong> We learn to move by
            interacting with the world, experimenting with different actions, and
            receiving feedback from our senses <a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>.<br>
          </li>
          <li><strong>Dynamic Systems:</strong> Our bodies are complex dynamic
            systems, constantly adapting and adjusting to maintain balance and
            achieve our goals <a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>.<br>
          </li>
          <li><strong>The Role of Perception:</strong> Perception is not just
            about passively receiving information; it’s an active process that
            guides our actions and shapes our understanding of the world <a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>.</li>
        </ul>
        <p>Replicating embodied cognition in robots is a significant challenge.
          It requires developing AI systems that can learn from physical
          interactions, adapt to changing environments, and integrate sensory
          information with motor control in a way that mirrors human intelligence.
          This differs significantly from information processing tasks, where the
          focus is on manipulating symbols and data within a defined system.
          Embodied cognition emphasizes the interconnectedness of the body, mind,
          and environment in shaping cognitive processes <a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>.</p>
        <h2 id="conclusion"><strong>Conclusion</strong></h2>
        <p>While AI has made remarkable progress in text generation and
          information processing, replicating mechanical motion remains a more
          formidable challenge. The complexities of the physical world, the
          infinite degrees of freedom in human movement, and the role of embodied
          cognition all contribute to this difficulty. However, ongoing research
          in robotics, AI, and embodied cognitive science promises to bring us
          closer to creating robots that can move with the fluidity, adaptability,
          and intelligence of humans and animals.</p>
        <p>This research highlights the need for continued exploration in areas
          such as:</p>
        <ul>
          <li>Developing more advanced actuators and artificial muscles that can
            better mimic the capabilities of biological systems <a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>.<br>
          </li>
          <li>Improving sensor technology and integration to provide robots with
            more comprehensive and accurate environmental awareness <a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>.<br>
          </li>
          <li>Designing AI algorithms that can learn from physical interactions
            and adapt to dynamic environments <a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>.<br>
          </li>
          <li>Furthering our understanding of embodied cognition and how it can be
            applied to robotics <a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>.</li>
        </ul>
        <p>By addressing these challenges, we can pave the way for a future
          where robots can seamlessly integrate into our world, assisting us in a
          wide range of tasks and enhancing our lives in countless ways.</p>
        <h4 id="works-cited"><strong>Works cited</strong></h4>
        <section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
          <hr>
          <ol>
            <li id="fn1">
              <p>Generative AI that imitates human motion - ScienceDaily,
                accessed January 20, 2025, <a href="https://www.sciencedaily.com/releases/2024/05/240509110822.htm">https://www.sciencedaily.com/releases/2024/05/240509110822.htm</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn2">
              <p>Generative AI that imitates human motion - ScienceDaily,
                accessed January 20, 2025, <a href="https://www.sciencedaily.com/releases/2024/05/240509110822.htm">https://www.sciencedaily.com/releases/2024/05/240509110822.htm</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn3">
              <p>Generative AI that imitates human motion - ScienceDaily,
                accessed January 20, 2025, <a href="https://www.sciencedaily.com/releases/2024/05/240509110822.htm">https://www.sciencedaily.com/releases/2024/05/240509110822.htm</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn4">
              <p>Embodied cognitive science - Wikipedia, accessed January
                20, 2025, <a href="https://en.wikipedia.org/wiki/Embodied_cognitive_science">https://en.wikipedia.org/wiki/Embodied_cognitive_science</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn5">
              <p>The Future of Text Generation: Unleashing the Power of
                AI - AIContentfy, accessed January 20, 2025, <a href="https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai">https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn6">
              <p>The Future of Text Generation: Unleashing the Power of
                AI - AIContentfy, accessed January 20, 2025, <a href="https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai">https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn7">
              <p>The Future of Text Generation: Unleashing the Power of
                AI - AIContentfy, accessed January 20, 2025, <a href="https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai">https://aicontentfy.com/en/blog/future-of-text-generation-unleashing-power-of-ai</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn8">
              <p>The Evolution of Text-to-Text Generation Models: A
                Comprehensive Overview - Medium, accessed January 20, 2025, <a href="https://medium.com/the-it-bros-newsletter/the-evolution-of-text-to-text-generation-models-a-comprehensive-overview-592e7f7dfad9">https://medium.com/the-it-bros-newsletter/the-evolution-of-text-to-text-generation-models-a-comprehensive-overview-592e7f7dfad9</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn9">
              <p>Computational complexity theory - Wikipedia, accessed
                January 20, 2025, <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">https://en.wikipedia.org/wiki/Computational_complexity_theory</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn10">
              <p>Mechanical Computing: The Computational Complexity of
                Physical Devices, accessed January 20, 2025, <a href="https://users.cs.duke.edu/~reif/paper/MechComp/MechComp.pub.no.permission.pdf">https://users.cs.duke.edu/~reif/paper/MechComp/MechComp.pub.no.permission.pdf</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn11">
              <p>Mechanical Computing: The Computational Complexity of
                Physical Devices, accessed January 20, 2025, <a href="https://users.cs.duke.edu/~reif/paper/MechComp/MechComp.pub.no.permission.pdf">https://users.cs.duke.edu/~reif/paper/MechComp/MechComp.pub.no.permission.pdf</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn12">
              <p>Emerging innovations in electrically powered artificial
                muscle fibers - Oxford Academic, accessed January 20, 2025, <a href="https://academic.oup.com/nsr/article/11/10/nwae232/7708368">https://academic.oup.com/nsr/article/11/10/nwae232/7708368</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn13">
              <p>What are the challenges in the way of mass adoption of
                robotics? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/">https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn14">
              <p>Biomimetic Approaches for Human Arm Motion Generation:
                Literature Review and Future Directions - PubMed Central, accessed
                January 20, 2025, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/">https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn15">
              <p>Biomimetic Approaches for Human Arm Motion Generation:
                Literature Review and Future Directions - PubMed Central, accessed
                January 20, 2025, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/">https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn16">
              <p>Biomimetic Approaches for Human Arm Motion Generation:
                Literature Review and Future Directions - PubMed Central, accessed
                January 20, 2025, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/">https://pmc.ncbi.nlm.nih.gov/articles/PMC10143908/</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn17">
              <p>Human-Like Arm Motion Generation: A Review - MDPI,
                accessed January 20, 2025, <a href="https://www.mdpi.com/2218-6581/9/4/102">https://www.mdpi.com/2218-6581/9/4/102</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn18">
              <p>Human-Like Arm Motion Generation: A Review - MDPI,
                accessed January 20, 2025, <a href="https://www.mdpi.com/2218-6581/9/4/102">https://www.mdpi.com/2218-6581/9/4/102</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn19">
              <p>Human-Like Arm Motion Generation: A Review - MDPI,
                accessed January 20, 2025, <a href="https://www.mdpi.com/2218-6581/9/4/102">https://www.mdpi.com/2218-6581/9/4/102</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn20">
              <p>Human-Like Arm Motion Generation: A Review - MDPI,
                accessed January 20, 2025, <a href="https://www.mdpi.com/2218-6581/9/4/102">https://www.mdpi.com/2218-6581/9/4/102</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn21">
              <p>ImitationNet: Unsupervised Human-to-Robot Motion
                Retargeting via Shared Latent Space, accessed January 20, 2025, <a href="https://arxiv.org/html/2309.05310v3">https://arxiv.org/html/2309.05310v3</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn22">
              <p>Repeatability and Accuracy Myth - Southie Autonomy,
                accessed January 20, 2025, <a href="https://www.southie.ai/robotics-myth-1-repeatability-and-accuracy">https://www.southie.ai/robotics-myth-1-repeatability-and-accuracy</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn23">
              <p>Repeatability and Accuracy Myth - Southie Autonomy,
                accessed January 20, 2025, <a href="https://www.southie.ai/robotics-myth-1-repeatability-and-accuracy">https://www.southie.ai/robotics-myth-1-repeatability-and-accuracy</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn24">
              <p>Motion Replication | Request PDF - ResearchGate,
                accessed January 20, 2025, <a href="https://www.researchgate.net/publication/312861701_Motion_Replication">https://www.researchgate.net/publication/312861701_Motion_Replication</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn25">
              <p>What are the challenges in the way of mass adoption of
                robotics? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/">https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/</a><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn26">
              <p>What are the challenges in the way of mass adoption of
                robotics? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/">https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/</a><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn27">
              <p>What are the challenges in the way of mass adoption of
                robotics? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/">https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn28">
              <p>What are the biggest challenges to a self-replicating
                robot? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/299adl/what_are_the_biggest_challenges_to_a/">https://www.reddit.com/r/robotics/comments/299adl/what_are_the_biggest_challenges_to_a/</a><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn29">
              <p>Human-Like Arm Motion Generation: A Review - MDPI,
                accessed January 20, 2025, <a href="https://www.mdpi.com/2218-6581/9/4/102">https://www.mdpi.com/2218-6581/9/4/102</a><a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn30">
              <p>The Mechanics of Embodiment: A Dialog on Embodiment and
                Computational Modeling - PMC - PubMed Central, accessed January 20,
                2025, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3111422/">https://pmc.ncbi.nlm.nih.gov/articles/PMC3111422/</a><a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn31">
              <p>Embodied cognition: Enhancing thinking through movement
                and gesture - My College, accessed January 20, 2025, <a href="https://my.chartered.college/impact_article/embodied-cognition-enhancing-thinking-through-movement-and-gesture/">https://my.chartered.college/impact_article/embodied-cognition-enhancing-thinking-through-movement-and-gesture/</a><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn32">
              <p>The Burden of Embodied Cognition - PMC - PubMed
                Central, accessed January 20, 2025, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4701031/">https://pmc.ncbi.nlm.nih.gov/articles/PMC4701031/</a><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn33">
              <p>Embodied Cognition and the Simulation of Action to
                Understand Others, accessed January 20, 2025, <a href="https://labs.psych.ucsb.edu/grafton/scott/Papers/Grafton2009Annals%20of%20the%20New%20York%20Academy%20of%20Sciences.pdf">https://labs.psych.ucsb.edu/grafton/scott/Papers/Grafton2009Annals%20of%20the%20New%20York%20Academy%20of%20Sciences.pdf</a><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn34">
              <p>Integrating Embodied Cognition and Information
                Processing: A Combined Model of the Role of Gesture in Children’s
                Mathematical Environments - PubMed Central, accessed January 20, 2025,
                <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8062855/">https://pmc.ncbi.nlm.nih.gov/articles/PMC8062855/</a><a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a>
              </p>
            </li>
            <li id="fn35">
              <p>Emerging innovations in electrically powered artificial
                muscle fibers - Oxford Academic, accessed January 20, 2025, <a href="https://academic.oup.com/nsr/article/11/10/nwae232/7708368">https://academic.oup.com/nsr/article/11/10/nwae232/7708368</a><a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn36">
              <p>What are the challenges in the way of mass adoption of
                robotics? - Reddit, accessed January 20, 2025, <a href="https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/">https://www.reddit.com/r/robotics/comments/195k0lt/what_are_the_challenges_in_the_way_of_mass/</a><a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn37">
              <p>Generative AI that imitates human motion -
                ScienceDaily, accessed January 20, 2025, <a href="https://www.sciencedaily.com/releases/2024/05/240509110822.htm">https://www.sciencedaily.com/releases/2024/05/240509110822.htm</a><a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn38">
              <p>Embodied cognitive science - Wikipedia, accessed
                January 20, 2025, <a href="https://en.wikipedia.org/wiki/Embodied_cognitive_science">https://en.wikipedia.org/wiki/Embodied_cognitive_science</a><a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p>
            </li>
          </ol>
        </section>
        <div class="lastmodified">
          <p>Last modified: January 1, 2025</p>
        </div>
      </article>
      <div class="container">
        <nav class="navlike">
          <div class="fill">&nbsp;</div>
        </nav>
      </div>
    </main>
    <footer class="container">
      <h2>More posts from this blog ...</h2>
      <div class="post-card">
        <h3 class="title">
          <a href="/posts/identity-domains/">
            Identity Domains
          </a>
        </h3>
        <div class="navlike">
          <span class="date navlike">
            Last updated on May 1, 2020
        </span>
          <span class="fill"></span>
          <span class="tag-link">
    <a href="/tags/privacy/">#Privacy</a>
</span>
        </div>
        <div>
          <div class="summary">
            <p>An Identity Domain is a scope within which we assume that the user's identity
              can roam freely.</p>
          </div>
          <p class="more"><a href="/posts/identity-domains/">Read more …</a></p>
        </div>
      </div>
      <div class="post-card">
        <h3 class="title">
          <a href="/posts/visualizing-people/">
            Visualizing Internet Users
          </a>
        </h3>
        <div class="navlike">
          <span class="date navlike">
            Last updated on November 21, 2020
        </span>
          <span class="fill"></span>
          <span class="tag-link">
    <a href="/tags/engineering/">#Engineering</a>
</span>
          <span class="tag-link">
    <a href="/tags/curiosity/">#Curiosity</a>
</span>
        </div>
      </div>
      <div class="post-card">
        <h3 class="title">
          <a href="/posts/darpa-heilmeier-catechism/">
            DARPA&#39;s Heilmeier Catechism
          </a>
        </h3>
        <div class="navlike">
          <span class="date navlike">
            Last updated on June 28, 2022
        </span>
          <span class="fill"></span>
          <span class="tag-link">
    <a href="/tags/engineering/">#engineering</a>
</span>
        </div>
        <div>
          <div class="summary">
            <p>A set of questions help you think through and evaluate proposed projects and research programs.</p>
          </div>
          <p class="more"><a href="/posts/darpa-heilmeier-catechism/">Read more …</a></p>
        </div>
      </div>
      <div class="post-card">
        <h3 class="title">
          <a href="/posts/structure-of-experience/"><span class="series-name">Structure of Experience</span>:
            Preface
          </a>
        </h3>
        <div class="navlike">
          <span class="date navlike">
            Last updated on May 1, 2025
        </span>
          <span class="fill"></span>
          <span class="tag-link">
    <a href="/tags/ai/">#AI</a>
</span>
        </div>
        <div>
          <div class="summary">
            <p>A very gentle introduction to how large language models <em>almost</em> capture human experience through language.</p>
          </div>
          <p class="more"><a href="/posts/structure-of-experience/">Read more …</a></p>
        </div>
      </div>
      <div class="post-card">
        <h3 class="title">
          <a href="/posts/how-this-blog-works/">
            How This Blog Works
          </a>
        </h3>
        <div class="navlike">
          <span class="date navlike">
            Last updated on July 27, 2020
        </span>
          <span class="fill"></span>
          <span class="tag-link">
    <a href="/tags/meta/">#Meta</a>
</span>
        </div>
        <div>
          <div class="summary">
            <p>How I put this site together and why.</p>
          </div>
          <p class="more"><a href="/posts/how-this-blog-works/">Read more …</a></p>
        </div>
      </div>
    </footer>
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GSREL2EZ4Y"></script>
    <script>
      var dnt, doNotTrack = !1;
      if (!1 && (dnt = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack, doNotTrack = dnt == "1" || dnt == "yes"), !doNotTrack) {
        window.dataLayer = window.dataLayer || [];
        function gtag() {
          dataLayer.push(arguments)
        }
        gtag("js", new Date), gtag("config", "G-GSREL2EZ4Y")
      }
    </script>
  </body>
</html>