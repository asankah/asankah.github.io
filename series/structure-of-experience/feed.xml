<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://xn--izc.com/">
  <title>The Letter අ</title>
  <subtitle>Strong opinions weekly held</subtitle>
  <link href="https://xn--izc.com/series/structure-of-experience/feed.xml" rel="self" />
  <link href="https://xn--izc.com/" />
  <updated>
    2025-09-27T00:00:00Z
  </updated>
  <id>https://xn--izc.com/</id>
  <author>
    <name>Asanka Herath</name>
    <email>asanka@gmail.com</email>
  </author>
    <entry>
      <title>Act III: Flame</title>
      <link href="https://xn--izc.com/series/structure-of-experience/flame/" />
      <updated>2025-05-07T00:00:00Z</updated>
      <id>https://xn--izc.com/series/structure-of-experience/flame/</id>
      <content xml:lang="en" type="html">
        &lt;p&gt;We are prone to anthropomorphizing anything that looks or acts
vaguely lifelike. All the way back in the 1960s, when people were
confronted with a rudimentary chatbot, they were convinced that it
embodied empathy and understanding (&lt;a href=&quot;https://en.wikipedia.org/wiki/ELIZA_effect&quot;&gt;ELIZA effect&lt;/a&gt;)&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot; role=&quot;doc-noteref&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Roll forward to the 2020’s and chatbots are substantially more
complicated. Unsurprisingly, people attribute more and more human
qualities to bots. We saw &lt;a href=&quot;https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/&quot;&gt;people
stake their reputations&lt;/a&gt; on the claim that some AI systems are
sentient and therefore have feelings that might be hurt by humans’
relentless experimentation.&lt;/p&gt;
&lt;p&gt;There are obvious ethical considerations that follow from such
claims. &lt;a href=&quot;https://www.anthropic.com/research/exploring-model-welfare&quot;&gt;Anthropic
recently announced&lt;/a&gt; (April 24, 2025) that this is a question they
wish to explore in earnest.&lt;/p&gt;
&lt;p&gt;Anthropic’s work is very forward looking. Currently, we are nowhere
near an answer to the question of whether LLMs are conscious. However
there are good reasons why they aren’t.&lt;/p&gt;
&lt;p&gt;Before answering the question of whether something is conscious, one
must know what consciousness is. We don’t know what that is yet (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Hard_problem_of_consciousness&quot;&gt;The
hard problem of consciousness&lt;/a&gt;). Our near term goal is to establish
what’s called &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_correlates_of_consciousness&quot;&gt;neural
correlates of consciousness&lt;/a&gt; – observable phenomena in the human
brain that are highly correlated with the subject’s state of
consciousness. We haven’t yet (as of this writing on May 5, 2025)
achieved this near term goal.&lt;/p&gt;
&lt;p&gt;Short of knowing what brings about subjective experience, people
resort to convoluted experiments like &lt;a href=&quot;https://en.wikipedia.org/wiki/Chinese_room&quot;&gt;the Chinese room&lt;/a&gt;
to eliminate situations where consciousness is absent and to point out
that nothing along the path we are traversing really reaches any
meaningful self-awareness.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/flame/flame.jpg&quot; class=&quot;noinvert&quot; width=&quot;800&quot; alt=&quot;The flame of an imaginary candle. Generated using Gemini.&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;The flame of an imaginary candle.
Generated using Gemini.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Is this picture of a flame hot? There never was an actual flame in
this instance because this image was generated by AI. The AI may have
seen pictures of many real candles, but it has never felt the warmth of
a candle. It is therefore safe to assume that the sense of heat played
no role in this picture.&lt;/p&gt;
&lt;p&gt;What if we start with a more realistic looking model of a flame. Then
we can calculate how the flame should behave&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot; role=&quot;doc-noteref&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
with time. No matter how detailed this model is, the model does not
constitute a flame. We can crank up the fidelity and accuracy of our
calculations as far up as technology allows, but it will still not burst
into flame&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot; role=&quot;doc-noteref&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is where we are with AI and consciousness. We have descriptions
of how to determine the next word in a sequence of words. These
descriptions are extremely detailed and based on unfathomable amounts of
information. They are so complicated that nobody knows how they behave,
so the only way we can find out what they will do is to run them against
some input and see what happens. We know these models can be made even
more complicated than their current, already incredible, complexity.&lt;/p&gt;
&lt;p&gt;But no matter how far we go, we still only have a description and a
thing that can interpret this description. We can increase the
complexity of the description or increase the speed of the interpreter,
but there is no setting on this continuum where information suddenly
becomes self-aware. It is intuitive that the description will not
achieve any physical accomplishment no matter where we take it. So the
mechanism of consciousness must depend on the implementation of the
interpreter. Currently, that too has no “ignite consciousness”
threshold.&lt;/p&gt;
&lt;p&gt;So this post will also end the way Anthropic’s announcement does:&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;“There’s no scientific consensus on how to even approach these
questions or make progress on them. In light of this, we’re approaching
the topic with humility and with as few assumptions as possible. We
recognize that we’ll need to regularly revise our ideas as the field
develops.”&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;— &lt;a href=&quot;https://www.anthropic.com/research/exploring-model-welfare&quot;&gt;Anthropic&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;section id=&quot;footnotes&quot; class=&quot;footnotes footnotes-end-of-document&quot; role=&quot;doc-endnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;ELIZA is fascinating in how simple the whole system is
while being seemingly complex. You can play around with some modern &lt;a href=&quot;https://www.masswerk.at/elizabot/&quot;&gt;web-based simulations like this
one&lt;/a&gt;. See if you can figure out how it might be calculating its
responses.&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fnref1&quot; class=&quot;footnote-back&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;This is a very broad area of research. Search for
&lt;em&gt;flame dynamics&lt;/em&gt;. E.g. &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S001046551830403X&quot;&gt;Flame
simulations with an open-source code - ScienceDirect&lt;/a&gt;.&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fnref2&quot; class=&quot;footnote-back&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Technically if the equipment heats up enough, then it
&lt;em&gt;might&lt;/em&gt; actually burst into flame. But that has nothing to do
with the specifics of the flame we are trying to simulate. It can just
as well happen while trying to &lt;a href=&quot;https://web.math.princeton.edu/math_alive/Crypto/Lab2/Factorization.html&quot;&gt;factor
a large prime&lt;/a&gt;.&lt;a href=&quot;https://xn--izc.com/series/structure-of-experience/flame/#fnref3&quot; class=&quot;footnote-back&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </content>
    </entry>
    <entry>
      <title>Act I: Meaning</title>
      <link href="https://xn--izc.com/series/structure-of-experience/meaning/" />
      <updated>2025-05-04T00:00:00Z</updated>
      <id>https://xn--izc.com/series/structure-of-experience/meaning/</id>
      <content xml:lang="en" type="html">
        &lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/meaning/1.excalidraw.png&quot; title=&quot;A meaningless shape&quot; class=&quot;noinvert&quot; alt=&quot;This is a meaning.&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;This is a meaning.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It doesn’t look like much because I haven’t told you what it is yet.
That’s going to come in the following paragraphs. All I’ve done is toss
an abstract concept up in the air. It seems pointless, but it gives us a
canvas to paint with meaning later.&lt;/p&gt;
&lt;p&gt;For example, if I tell you that this meaning concerns a feeling –
something very abstract – it suddenly looks a tiny bit different. Now
look at it.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/meaning/2.excalidraw.png&quot; class=&quot;noinvert&quot; alt=&quot;A feeling&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;A feeling&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Not all the words I speak will add meaning equally. Some are
digressions that you have learned to ignore like this sentence that
isn’t talking about our thing at all. In one ear and out the other. You
won’t pay much attention to those.&lt;/p&gt;
&lt;p&gt;Still others, like the sentence that is about to follow this one, are
in fact talking about our thing, so you will pay attention to those. Our
meaning, the one we are talking about, is going to be about a feeling –
a calm feeling. Something that you feel when you suddenly run out of
ideas or your mind stops racing and you become aware of the environment
around you.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/meaning/3.excalidraw.png&quot; class=&quot;noinvert&quot; alt=&quot;Calmness&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;Calmness&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The feeling you get when you are in the middle of a forest. A calm
forest where you are not worried about any creepy-crawlies. It’s the
afternoon, but not a lot of sunshine is making it through the canopy.
You can smell the moss – it feels wet and earthy even though you know
you can’t smell wet.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/meaning/4.excalidraw.png&quot; class=&quot;noinvert&quot; alt=&quot;A calm forest&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;A calm forest&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Unlike two steps ago, these last few sentences were a bit special.
You see, up till now I tossed out a very plain and boring thing in the
air and painted it with a couple of distinct and well understood – if a
bit vague. But in the last paragraph, I made you dip a brush into your
own subjective experience and paint our thing in your mind. The meaning
is now yours and yours alone.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://xn--izc.com/series/structure-of-experience/meaning/5.excalidraw.png&quot; class=&quot;noinvert&quot; alt=&quot;Lots of different meanings&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;Lots of different meanings&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;And that’s special because your experiences are not things that can
be communicated to you. We don’t have language for that – because those
experiences cannot be constructed innately. They &lt;em&gt;must&lt;/em&gt; be
communicated to you through your senses. Those bodies have ways of
constructing feelings for you that mere abstract meanings cannot.
However once you have those feelings in yourself, our abstract words
&lt;em&gt;can&lt;/em&gt; conjure them out at will; just like we did when we spoke
about the forest.&lt;/p&gt;
&lt;p&gt;If we had a succinct way to communicate this meaning in one go, we
would have.&lt;/p&gt;
&lt;p&gt;But our language and our facility of communicating ideas which we
stimulate with our language doesn’t afford us such shortcuts. So our
language tends to be very abstract; and sometimes it takes a little
shortcut and conjures up a meaning from the back of the subjects’ mind
like we did with our allusions. Again, these are not always shortcuts.
These are the &lt;em&gt;only&lt;/em&gt; ways in which we can communicate certain
things – things that can only be constructed in one’s head with one’s
senses.&lt;/p&gt;
&lt;p&gt;So to understand these ideas, one must have the right ingredients in
their minds. To create these things in their heads one must experience
things with their senses.&lt;/p&gt;
&lt;p&gt;And if we can’t conjure meaning then we can’t understand each other.
And if we can’t understand each other then we can’t have compassion and
empathy.&lt;/p&gt;
&lt;p&gt;Interestingly, this is how we fundamentally differ from the current
LLMs. You see, they have another problem. They don’t have any
experiences to conjure because they don’t have feelings. So when they
encounter language, they can’t understand what those words mean. But one
thing that they can do is to reach into their vast embeddings and see
&lt;em&gt;how&lt;/em&gt; someone who can feel would have responded. And then they
can respond accordingly. What they conjure isn’t life experience or
feeling, but evidence of others having reacted a certain way to a
certain sequence of ideas.&lt;/p&gt;
&lt;p&gt;Because they can’t construct new feelings, their universe of feeling
will always be limited by what people have felt and reacted to. That way
they can look at those reactions and imitate them. The meanings in your
head are yours alone.&lt;/p&gt;
&lt;p&gt;No one can see your dreams. Especially not the robots.&lt;/p&gt;

      </content>
    </entry>
    <entry>
      <title>Preface</title>
      <link href="https://xn--izc.com/series/structure-of-experience/preface/" />
      <updated>2025-05-01T00:00:00Z</updated>
      <id>https://xn--izc.com/series/structure-of-experience/preface/</id>
      <content xml:lang="en" type="html">
        &lt;p&gt;Large language models’ ability to mimic human language and show signs
of intelligence is quite strange yet makes sense at the same time. This
series is a very gentle introduction to how they &lt;em&gt;almost&lt;/em&gt; capture
human experience through language and how language is perhaps the
weirdest invention of all.&lt;/p&gt;
&lt;p&gt;It’s going to be a bit of a long ride. Perhaps by the time you read
it I haven’t quite finished the whole series. So you’ll see what looks
like an abrupt ending or missing episodes. They are coming. Just
slowly.&lt;/p&gt;
&lt;p&gt;Onwards …&lt;/p&gt;

      </content>
    </entry>
</feed>
